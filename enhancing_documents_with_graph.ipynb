{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89raxIOuzyIN",
        "outputId": "5aec6727-1315-4385-c936-354393418c68"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.1-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.1-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from neo4j import GraphDatabase\n",
        "import fitz  # PyMuPDF for PDF text extraction\n",
        "\n",
        "# Neo4j Integration\n",
        "class Neo4jHandler:\n",
        "    def __init__(self, uri, user, password, database):\n",
        "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
        "        self.database = database\n",
        "\n",
        "    def close(self):\n",
        "        self.driver.close()\n",
        "\n",
        "    def store_chunks(self, chunks, relationships):\n",
        "        \"\"\"Store chunks as nodes and establish relationships.\"\"\"\n",
        "        with self.driver.session(database=self.database) as session:\n",
        "            for chunk_id, chunk_text in chunks.items():\n",
        "                # Create nodes for each chunk\n",
        "                session.run(\n",
        "                    \"\"\"\n",
        "                    MERGE (c:Chunk {id: $chunk_id, text: $chunk_text})\n",
        "                    \"\"\",\n",
        "                    chunk_id=chunk_id,\n",
        "                    chunk_text=chunk_text,\n",
        "                )\n",
        "            for rel in relationships:\n",
        "                # Create relationships between relevant chunks\n",
        "                session.run(\n",
        "                    \"\"\"\n",
        "                    MATCH (c1:Chunk {id: $source_id})\n",
        "                    MATCH (c2:Chunk {id: $target_id})\n",
        "                    MERGE (c1)-[:RELATED]->(c2)\n",
        "                    \"\"\",\n",
        "                    source_id=rel[\"source\"],\n",
        "                    target_id=rel[\"target\"],\n",
        "                )\n",
        "\n",
        "# PDF Text Extraction\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extract text from a PDF file.\"\"\"\n",
        "    text = \"\"\n",
        "    with fitz.open(pdf_path) as pdf:\n",
        "        for page in pdf:\n",
        "            text += page.get_text()\n",
        "    return text\n",
        "\n",
        "# Text Splitting and Similarity\n",
        "def split_and_analyze_document(file_path):\n",
        "    \"\"\"Split document and find relationships.\"\"\"\n",
        "    # Extract text from the PDF\n",
        "    document = extract_text_from_pdf(file_path)\n",
        "\n",
        "    # Split document into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "    chunks = text_splitter.split_text(document)\n",
        "\n",
        "    # Assign unique IDs to chunks\n",
        "    chunk_dict = {f\"chunk_{i}\": chunk for i, chunk in enumerate(chunks)}\n",
        "\n",
        "    # Use Sentence Transformers for embeddings\n",
        "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    embeddings = model.encode(list(chunk_dict.values()), convert_to_tensor=True)\n",
        "\n",
        "    # Find relationships based on cosine similarity\n",
        "    relationships = []\n",
        "    for i, source_embedding in enumerate(embeddings):\n",
        "        cos_similarities = util.cos_sim(source_embedding, embeddings)\n",
        "        for j, score in enumerate(cos_similarities[0]):\n",
        "            if i != j and score > 0.8:  # Threshold for similarity\n",
        "                relationships.append({\"source\": f\"chunk_{i}\", \"target\": f\"chunk_{j}\"})\n",
        "\n",
        "    return chunk_dict, relationships\n",
        "\n",
        "# Initialize Neo4j handler\n",
        "neo4j_handler = Neo4jHandler(\n",
        "    uri=\"bolt://3.235.154.204\",\n",
        "    user=\"neo4j\",\n",
        "    password=\"networks-centerline-symbols\",\n",
        "    database=\"neo4j\",\n",
        ")\n",
        "\n",
        "# Process the document and store in Neo4j\n",
        "file_path = \"/content/Redacted.pdf\"\n",
        "chunks, relationships = split_and_analyze_document(file_path)\n",
        "neo4j_handler.store_chunks(chunks, relationships)\n",
        "\n",
        "# Close Neo4j connection\n",
        "neo4j_handler.close()\n"
      ],
      "metadata": {
        "id": "9kTq1qCEVnu8"
      },
      "execution_count": 36,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}